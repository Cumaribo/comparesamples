# This code is used to download all the forest- no forest binary masks from the 
# Global forest cover Dataset by Hansen et al  using the package ForestChange.
#It  returns maps for each one of the thresholds between 70 and 100%. The next step is to load this, stack cut it by pieces and then carry out the comparson betwee nthe maps. Everything trough maps.

install.packages('unixtools', repos = 'http://www.rforge.net/')
getwd()
unixtools::set.tempdir("~/Documents/biomas_iavh")

library(raster)
library(rgdal)
library(forestChange)
library(parallel)
library(sf)
library(tidyverse)
library(purrr)
library(furrr)
library(diffeR)

dir.create('tempfiledir')
tempdir=paste(getwd(),'tempfiledir', sep="/")
rasterOptions(tmpdir=tempdir)
#Set your working folder
setwd("~/Documents/biomas_iavh")
dir()

# load data 
# mun <- st_read('~/folders/ContornoColombia.geojson')
# mun <- mun[!st_is_empty(mun),,drop=FALSE]

#make sure that your vectorfile is in crs=WGS84, as this is the one of the gobal forest dataset.

# if necessary, use this to reproject:
#mun <- spTransform(mun, crs='+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0')
#Do not do this here, it messes with the encoding of accents!!!!!!!!!!!!!!!!!!!!
#Sys.setlocale(locale="C")

# # convert map into spatial polygon dataframe.
# mun <- as(mun, 'Spatial')
# 
# # create vector withe the threshold you want to iterate. 
# # Warning, this process requires a lot of temp memory, so make sure to have enough storage space or split your process in smaller chuncks
# perc2 <- 70:100
# # this creates a vector of names from the vector
# perccar <- as.character(perc2) 
# # set a temporary working folder.


# # download each one of the maps. The allowable length of your vector depends on your available storage and the size of the study area 
# # i need to parallelize this. It is not running parallel when it comes to "Mosaicing is required". it might work with future_map
# test2 <- map(perc2, function(x) FCMask(mun, year=2017, cummask=FALSE, perc = x, mc.cores = 8)) 
# 
# #Save your rasters
# # parallelize this, to get it to write faster into memory
# map(1:length(perc2), function(x) writeRaster(test2[[x]], paste('forest_col', perccar[x], sep='_'), format='GTiff')) 

#load the biomas

mun <- st_read('~workingfolder/biomas_wgs84.shp')
labels <- (mun$BIOMA_IAvH)

mun <- as(mun, 'Spatial')
# split mun into list of independent polygons  
biomat <- mun%>%split(.$BIOMA_IAvH)

#######################################################################################################
#biomat <- biomat[-c(1:379)]
#biomat[[1]]
#load the rasters i just created. 
#create a list with the names of the masks
listr <- list.files(".", "forest_col")
#create an empty container list
r.list <- list()

# load the rasters
for(i in 1:length(listr)){
  r.list[[i]] <- raster(listr[i])}
# stack the rasters to build a multi band rasterstack. 
stack_forests <- do.call(stack, r.list)
rm(r.list)
# I should have done this here, but I missed it and can't repeat a procedure that took a whole week to run

#not siure, it uses a LOT of memory
# stack_forests[is.na(stack_forests)] <- 0
# stack_forests <- mask(stack_forests, mun)
#this is the next step. 
#Get to mask over all of the 399 pieces. 
#get the names
# attention, for memoy reasons, I had to split the thing in several stages, and thats why it. DON;'T FORGET to adjust this!!!!!!!!!!!!!
names <- as.list(mun$BIOMA_IAvH)
names <- map(1:length(names), function(x) as.character(names[[x]]))
namesu <- unlist(names)
#namesu <- namesu[379:386] #in case doing smaller chunks is required

#crop using the extent of each bioma
#here i will have a set of multiband rasters for each of the biomas

#prepare the cores. I am testing the furrr package to parallelize in a tidyverse map. My problem now is that the desktop at the lab has a tiny hard drive, and for some reason

#still dont send the stuff to process in nimbus, but keeps woking on the local machine. I decided to split it un parts for the moment
plan(multisession, workers=7)

  cropped <- future_map(1:length(namesu), function(x) crop(stack_forests, extent(biomat[[x]])))
  maskedt <- map(1:length(cropped), function(x) raster::mask(cropped[[x]], biomat[[x]]))
  map(1:length(maskedt), function(x) writeRaster(maskedt[[x]], paste('forests_2017', namesu[x], sep='_'), format='GTiff', overwrite=TRUE)) 

#########################################################################################################################


listr <- list.files(".", "forests_2017")
#create an empty container list
r.list <- list()
#load the stacks
for(i in 1:length(listr)){
  r.list[[i]] <- stack(listr[i])}

# # #create the list with the names 
# listnames <- list()
# for(i in 1:length(r.list)){
#   listnames[i] <- substr(r.list2[[i]]@file@name, 46, nchar(r.list2[[i]]@file@name))}
# listnames <- unlist(listnames)
# listnames <- as.factor(listnames)
# #i dont' need this anymore, so wewill get rid of it 
# rm(r.list2)

# set your future settings (number of cores and memory to be allocated)
mem_future <- 1000*1024^2 #this is toset the limit to 1GB
plan(multisession, workers=7)
options(future.globals.maxSize= mem_future)
# this function subsitutes the NA for zeros, masks the rasters and writes the new raster
masker <- function(test,poly, labels){
  test <- map(1:nlayers(test), function(x) replace_na(test[[x]],0))
  print('na-replaced')
  test2 <- do.call(stack,test)
  print('stacked')
  test2 <- mask(test2, poly)
  print('masked, writing raster')
  writeRaster(test2, paste('masked', labels, sep='_'), format='GTiff', overwrite=TRUE)
  return(test2)}

test2 <- future_map(1:length(labels), function(x) masker(r.list[[x]], biomat[[x]], labels[x]))

ideam <- raster('/Users/sputnik/Documents/Bosques_Victor/Bosque_No_Bosque_2017/Bosque_No_Bosque_2017/Geotiff/SBQ_SMBYC_BQNBQ_V7_2017.tif')
#load IDEAM Map.
plot(ideam)
m <- c(-Inf, 0, NA)
m <- matrix(m, ncol=3, byrow=TRUE)

ideam <- reclassify(ideam, m)
plot(ideam)
writeRaster(ideam, 'ideam_2017_masked', format='GTiff')

setwd('/media/mnt/Ecosistemas_Colombia')
dir()
ideam <- raster('~/Ecosistemas_Colombia/ideam_2017_masked.tif')


m <- c(1.1,2,0, 2.9, Inf, NA)
m <- matrix(m,ncol=3,byrow=TRUE)
ideam <- reclassify(ideam,m)
plot(ideam)
writeRaster(ideam, 'ideam_2017_reclass', format='GTiff')

ideam <- raster('ideam2017_reclass.tif')
hansen <- raster('forest_col_70.tif')

ideam <- resample(ideam, hansen)
ideam <- round(ideam, digits=0)

#mask the IDEAM map to each one of the stacks.
plan(multisession, workers=7)
#this is not working on the Mac on parallel, only trough normal mappimg. Will have to vheck in the Linux machine if it
# works. 
# cropped <- future_map(1:length(namesu), function(x) crop(ideam, extent(biomat[[x]])))
# plan(multisession, workers=15)
# maskedt <- future_map(1:length(cropped), function(x) raster::mask(cropped[[x]], biomat[[x]]))
# plan(multisession, workers=15)
# future_map(1:length(maskedt), function(x) writeRaster(maskedt[[x]], paste('ideam_2017', namesu[x], sep='_'), format='GTiff', overwrite=TRUE)) 

#it worked well like this
cropped <- map(1:length(namesu), function(x) crop(ideam, extent(biomat[[x]])))
maskedt <- map(1:length(cropped), function(x) raster::mask(cropped[[x]], biomat[[x]]))
map(1:length(maskedt), function(x) writeRaster(maskedt[[x]], paste('ideam_2017', namesu[x], sep='_'), format='GTiff', overwrite=TRUE)) 


# load the ideam maps 
listr <- list.files(".", "ideam_2017")
#create an empty container list

ideam <- list()

for(i in 1:length(listr)){
  ideam[[i]] <- raster(listr[i])}
# stack the rasters to build a multi band rasterstack. 

#load the masked Rasters
rm(list=ls())
setwd('~/Documents/biomas_iavh/masked')

dir()
hansen <- list()
listr <- list.files(".", "masked")
# load the Ideam rasters
for(i in 1:length(listr)){
  hansen[[i]] <- stack(listr[i])}

# Invert the bands because R
inversor <- function(test){
test <- stack(test[[2:30]], test[[1]])}
hansen <- future_map(1:length(hansen), function(x) inversor(hansen[[x]]))


#Compare maps

comparer <- function(ideam, hansen){
  test1 <- map(1:30, function(x) crosstabm(ideam[[1]], hansen[[1]][[x]], percent=TRUE, population=NULL))}

test_t <- crosstabm(ideam_t, hansen_t, percent=FALSE, population = NULL)

comparer <- function(ideam,hansen, perc){
  ideam_t <- ideam
  hansen_t <- hansen
  mem_future <- 1000*1024^2 #this is toset the limit to 1GB
  plan(multisession, workers=14)
  options(future.globals.maxSize= mem_future)
  test1 <- future_map(1:nlayers(hansen_t), function(x) crosstabm(ideam_t, hansen_t[[x]], percent=perc, population=NULL))
  print('biome(s) compared')
  differences <- future_map(1:length(test1), function(x) diffTablej(test1[[x]], digits = 2, analysis = 'error'))
  return(differences)}

mem_future <- 1000*1024^2 #this is toset the limit to 1GB
plan(multisession, workers=14)
options(future.globals.maxSize= mem_future)

diff_mat <- future_map(1:length(ideam), function(x) comparer(ideam[[x]], hansen[[x]], perc=TRUE))
names(diff_mat) <- labels

save(diff_mat, file='differences_biomas.Rdata')
write_csv(diff_mat, 'diff_mat.csv', col_names = TRUE)

save(diff_mat, file='differences_biomas_perc.Rdata')
write_csv(diff_mat, 'diff_mat_perc.csv', col_names = TRUE)
